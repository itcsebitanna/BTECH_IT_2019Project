{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                    # pandas is a dataframe library\n",
    "import matplotlib.pyplot as plt        # matplotlib.pyplot plot data \n",
    "import numpy as np           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " df = pd.read_csv(\"dataset\\diabetes_csv.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>insu</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preg  plas  pres  skin  insu  mass   pedi  age  class\n",
       "0     6   148    72    35     0  33.6  0.627   50   True\n",
       "1     1    85    66    29     0  26.6  0.351   31  False\n",
       "2     8   183    64     0     0  23.3  0.672   32   True\n",
       "3     1    89    66    23    94  28.1  0.167   21  False\n",
       "4     0   137    40    35   168  43.1  2.288   33   True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>insu</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     preg  plas  pres  skin  insu  mass   pedi  age  class\n",
       "763    10   101    76    48   180  32.9  0.171   63  False\n",
       "764     2   122    70    27     0  36.8  0.340   27  False\n",
       "765     5   121    72    23   112  26.2  0.245   30  False\n",
       "766     1   126    60     0     0  30.1  0.349   47   True\n",
       "767     1    93    70    31     0  30.4  0.315   23  False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df.isnull().values.any() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(df, size=10): \n",
    "    corr = df.corr()  # data frame corelation function  \n",
    "    fig , ax = plt.subplots(figsize =(size,size))   \n",
    "    ax.matshow(corr)  # color code the rectanges by corelation value    \n",
    "    plt.xticks (range(len(corr.columns)), corr.columns)    # draw x ticks mark   \n",
    "    plt.yticks (range(len(corr.columns)), corr.columns)    # draw y ticks mark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " plot_corr(df) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>insu</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>preg</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.129459</td>\n",
       "      <td>0.141282</td>\n",
       "      <td>-0.081672</td>\n",
       "      <td>-0.073535</td>\n",
       "      <td>0.017683</td>\n",
       "      <td>-0.033523</td>\n",
       "      <td>0.544341</td>\n",
       "      <td>0.221898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plas</th>\n",
       "      <td>0.129459</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.152590</td>\n",
       "      <td>0.057328</td>\n",
       "      <td>0.331357</td>\n",
       "      <td>0.221071</td>\n",
       "      <td>0.137337</td>\n",
       "      <td>0.263514</td>\n",
       "      <td>0.466581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pres</th>\n",
       "      <td>0.141282</td>\n",
       "      <td>0.152590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.207371</td>\n",
       "      <td>0.088933</td>\n",
       "      <td>0.281805</td>\n",
       "      <td>0.041265</td>\n",
       "      <td>0.239528</td>\n",
       "      <td>0.065068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skin</th>\n",
       "      <td>-0.081672</td>\n",
       "      <td>0.057328</td>\n",
       "      <td>0.207371</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.436783</td>\n",
       "      <td>0.392573</td>\n",
       "      <td>0.183928</td>\n",
       "      <td>-0.113970</td>\n",
       "      <td>0.074752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insu</th>\n",
       "      <td>-0.073535</td>\n",
       "      <td>0.331357</td>\n",
       "      <td>0.088933</td>\n",
       "      <td>0.436783</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.197859</td>\n",
       "      <td>0.185071</td>\n",
       "      <td>-0.042163</td>\n",
       "      <td>0.130548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mass</th>\n",
       "      <td>0.017683</td>\n",
       "      <td>0.221071</td>\n",
       "      <td>0.281805</td>\n",
       "      <td>0.392573</td>\n",
       "      <td>0.197859</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.140647</td>\n",
       "      <td>0.036242</td>\n",
       "      <td>0.292695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pedi</th>\n",
       "      <td>-0.033523</td>\n",
       "      <td>0.137337</td>\n",
       "      <td>0.041265</td>\n",
       "      <td>0.183928</td>\n",
       "      <td>0.185071</td>\n",
       "      <td>0.140647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033561</td>\n",
       "      <td>0.173844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.544341</td>\n",
       "      <td>0.263514</td>\n",
       "      <td>0.239528</td>\n",
       "      <td>-0.113970</td>\n",
       "      <td>-0.042163</td>\n",
       "      <td>0.036242</td>\n",
       "      <td>0.033561</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.238356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <td>0.221898</td>\n",
       "      <td>0.466581</td>\n",
       "      <td>0.065068</td>\n",
       "      <td>0.074752</td>\n",
       "      <td>0.130548</td>\n",
       "      <td>0.292695</td>\n",
       "      <td>0.173844</td>\n",
       "      <td>0.238356</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           preg      plas      pres      skin      insu      mass      pedi  \\\n",
       "preg   1.000000  0.129459  0.141282 -0.081672 -0.073535  0.017683 -0.033523   \n",
       "plas   0.129459  1.000000  0.152590  0.057328  0.331357  0.221071  0.137337   \n",
       "pres   0.141282  0.152590  1.000000  0.207371  0.088933  0.281805  0.041265   \n",
       "skin  -0.081672  0.057328  0.207371  1.000000  0.436783  0.392573  0.183928   \n",
       "insu  -0.073535  0.331357  0.088933  0.436783  1.000000  0.197859  0.185071   \n",
       "mass   0.017683  0.221071  0.281805  0.392573  0.197859  1.000000  0.140647   \n",
       "pedi  -0.033523  0.137337  0.041265  0.183928  0.185071  0.140647  1.000000   \n",
       "age    0.544341  0.263514  0.239528 -0.113970 -0.042163  0.036242  0.033561   \n",
       "class  0.221898  0.466581  0.065068  0.074752  0.130548  0.292695  0.173844   \n",
       "\n",
       "            age     class  \n",
       "preg   0.544341  0.221898  \n",
       "plas   0.263514  0.466581  \n",
       "pres   0.239528  0.065068  \n",
       "skin  -0.113970  0.074752  \n",
       "insu  -0.042163  0.130548  \n",
       "mass   0.036242  0.292695  \n",
       "pedi   0.033561  0.173844  \n",
       "age    1.000000  0.238356  \n",
       "class  0.238356  1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df.corr() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>insu</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preg  plas  pres  skin  insu  mass   pedi  age  class\n",
       "0     6   148    72    35     0  33.6  0.627   50   True\n",
       "1     1    85    66    29     0  26.6  0.351   31  False\n",
       "2     8   183    64     0     0  23.3  0.672   32   True\n",
       "3     1    89    66    23    94  28.1  0.167   21  False\n",
       "4     0   137    40    35   168  43.1  2.288   33   True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " class_map = {True:1 , False:0} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " df['class'] = df['class'].map(class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>insu</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preg  plas  pres  skin  insu  mass   pedi  age  class\n",
       "0     6   148    72    35     0  33.6  0.627   50      1\n",
       "1     1    85    66    29     0  26.6  0.351   31      0\n",
       "2     8   183    64     0     0  23.3  0.672   32      1\n",
       "3     1    89    66    23    94  28.1  0.167   21      0\n",
       "4     0   137    40    35   168  43.1  2.288   33      1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df.head(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "feature_col_names = ['preg', 'plas' , 'pres', 'insu', 'skin', 'mass', 'pedi', 'age']\n",
    "predicted_class_names = ['class'] \n",
    "X = df[feature_col_names].values # predictor feature coloumns ( 8 X m )\n",
    "y = df[predicted_class_names].values # predicted class ( 1= true , 0=false ) column ( 1 X m )\n",
    "split_test_size = 0.30 \n",
    "X_train , X_test , y_train , y_test = train_test_split(X, y, test_size = split_test_size, random_state = 42)           # test size = 0.30 is 30% , 42 is the answer to everything , any number can be used for ran dom state \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.92% in training set\n",
      "30.08% in test set\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:0.2f}% in training set\".format((len(X_train)/len(df.index))*100))\n",
    "print(\"{0:0.2f}% in test set\".format((len(X_test)/len(df.index))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origional True: 268 (34.90%)\n",
      "Origional False: 500 (65.10%)\n",
      " \n",
      "Training True: 188 (35.01%)\n",
      "Training False: 349 (64.99%)\n",
      " \n",
      "Testing True: 80 (34.63%)\n",
      "Testing False: 151 (65.37%)\n"
     ]
    }
   ],
   "source": [
    " print (\"Origional True: {0} ({1:0.2f}%)\".format(len(df.loc[df['class'] == 1]),(len(df.loc[df['class'] == 1]) / len(df.index))*100.0)) \n",
    "print (\"Origional False: {0} ({1:0.2f}%)\".format(len(df.loc[df['class'] == 0]),(len(df.loc[df['class'] == 0]) / len(df.index))*100.0))\n",
    "print(\" \")\n",
    "print (\"Training True: {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 1]),(len(y_train[y_train[:] == 1]) / len(y_train))*100.0))\n",
    "print (\"Training False: {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 0]),(len(y_train[y_train[:] == 0]) / len(y_train))*100.0))\n",
    "print(\" \")\n",
    "print (\"Testing True: {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 1]),(len(y_test[y_test[:] == 1]) / len(y_test))*100.0)) \n",
    "print (\"Testing False: {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 0]),(len(y_test[y_test[:] == 0]) / len(y_test))*100.0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows in dataframe 768\n",
      "# rows missing glucose_conc: 5\n",
      "# rows missing diastolic_bp: 35\n",
      "# rows missing thickness: 227\n",
      "# rows missing insulin: 374\n",
      "# rows missing bmi: 11\n",
      "# rows missing diab_pred: 0\n",
      "# rows missing age: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"# rows in dataframe {0}\".format(len(df))) \n",
    "print(\"# rows missing glucose_conc: {0}\".format(len(df.loc[df['plas'] == 0 ]))) \n",
    "print(\"# rows missing diastolic_bp: {0}\".format(len(df.loc[df['pres'] == 0 ])))\n",
    "print(\"# rows missing thickness: {0}\".format(len(df.loc[df['skin'] == 0 ]))) \n",
    "print(\"# rows missing insulin: {0}\".format(len(df.loc[df['insu'] == 0 ]))) \n",
    "print(\"# rows missing bmi: {0}\".format(len(df.loc[df['mass'] == 0 ])))\n",
    "print(\"# rows missing diab_pred: {0}\".format(len(df.loc[df['pedi'] == 0 ]))) \n",
    "print(\"# rows missing age: {0}\".format(len(df.loc[df['age'] == 0 ]))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer #impute with mean all 0 readings \n",
    "fill_0 = Imputer(missing_values = 0 , strategy =\"mean\", axis=0) \n",
    "X_train = fill_0.fit_transform(X_train) \n",
    "X_test = fill_0.fit_transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>543.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.191</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.537</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.1</td>\n",
       "      <td>1.441</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>846.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.587</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.484</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0.551</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.183</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>34.6</td>\n",
       "      <td>0.529</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>39.3</td>\n",
       "      <td>0.704</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.4</td>\n",
       "      <td>0.388</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>0.451</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.263</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>11.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>36.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>31.1</td>\n",
       "      <td>0.205</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.4</td>\n",
       "      <td>0.257</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>0.487</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.1</td>\n",
       "      <td>0.337</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>2.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>36.6</td>\n",
       "      <td>0.453</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>1.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.5</td>\n",
       "      <td>0.293</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>11.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>42.3</td>\n",
       "      <td>0.785</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>3.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.8</td>\n",
       "      <td>0.400</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>1.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>0.219</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>9.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.7</td>\n",
       "      <td>0.734</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>13.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1.174</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>12.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.488</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>1.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>49.3</td>\n",
       "      <td>0.358</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>1.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>46.3</td>\n",
       "      <td>1.096</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>3.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>36.4</td>\n",
       "      <td>0.408</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>6.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.178</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>4.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.2</td>\n",
       "      <td>1.182</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>1.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.261</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>3.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.223</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>0.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.222</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>8.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.4</td>\n",
       "      <td>0.443</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>1.057</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>7.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.391</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>0.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.3</td>\n",
       "      <td>0.258</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>1.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>0.197</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>6.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.5</td>\n",
       "      <td>0.278</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>2.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>28.4</td>\n",
       "      <td>0.766</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>9.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.403</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>9.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>0.142</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1     2      3     4     5      6     7\n",
       "0     6.0  148.0  72.0    0.0  35.0  33.6  0.627  50.0\n",
       "1     1.0   85.0  66.0    0.0  29.0  26.6  0.351  31.0\n",
       "2     8.0  183.0  64.0    0.0   0.0  23.3  0.672  32.0\n",
       "3     1.0   89.0  66.0   94.0  23.0  28.1  0.167  21.0\n",
       "4     0.0  137.0  40.0  168.0  35.0  43.1  2.288  33.0\n",
       "5     5.0  116.0  74.0    0.0   0.0  25.6  0.201  30.0\n",
       "6     3.0   78.0  50.0   88.0  32.0  31.0  0.248  26.0\n",
       "7    10.0  115.0   0.0    0.0   0.0  35.3  0.134  29.0\n",
       "8     2.0  197.0  70.0  543.0  45.0  30.5  0.158  53.0\n",
       "9     8.0  125.0  96.0    0.0   0.0   0.0  0.232  54.0\n",
       "10    4.0  110.0  92.0    0.0   0.0  37.6  0.191  30.0\n",
       "11   10.0  168.0  74.0    0.0   0.0  38.0  0.537  34.0\n",
       "12   10.0  139.0  80.0    0.0   0.0  27.1  1.441  57.0\n",
       "13    1.0  189.0  60.0  846.0  23.0  30.1  0.398  59.0\n",
       "14    5.0  166.0  72.0  175.0  19.0  25.8  0.587  51.0\n",
       "15    7.0  100.0   0.0    0.0   0.0  30.0  0.484  32.0\n",
       "16    0.0  118.0  84.0  230.0  47.0  45.8  0.551  31.0\n",
       "17    7.0  107.0  74.0    0.0   0.0  29.6  0.254  31.0\n",
       "18    1.0  103.0  30.0   83.0  38.0  43.3  0.183  33.0\n",
       "19    1.0  115.0  70.0   96.0  30.0  34.6  0.529  32.0\n",
       "20    3.0  126.0  88.0  235.0  41.0  39.3  0.704  27.0\n",
       "21    8.0   99.0  84.0    0.0   0.0  35.4  0.388  50.0\n",
       "22    7.0  196.0  90.0    0.0   0.0  39.8  0.451  41.0\n",
       "23    9.0  119.0  80.0    0.0  35.0  29.0  0.263  29.0\n",
       "24   11.0  143.0  94.0  146.0  33.0  36.6  0.254  51.0\n",
       "25   10.0  125.0  70.0  115.0  26.0  31.1  0.205  41.0\n",
       "26    7.0  147.0  76.0    0.0   0.0  39.4  0.257  43.0\n",
       "27    1.0   97.0  66.0  140.0  15.0  23.2  0.487  22.0\n",
       "28   13.0  145.0  82.0  110.0  19.0  22.2  0.245  57.0\n",
       "29    5.0  117.0  92.0    0.0   0.0  34.1  0.337  38.0\n",
       "..    ...    ...   ...    ...   ...   ...    ...   ...\n",
       "738   2.0   99.0  60.0  160.0  17.0  36.6  0.453  21.0\n",
       "739   1.0  102.0  74.0    0.0   0.0  39.5  0.293  42.0\n",
       "740  11.0  120.0  80.0  150.0  37.0  42.3  0.785  48.0\n",
       "741   3.0  102.0  44.0   94.0  20.0  30.8  0.400  26.0\n",
       "742   1.0  109.0  58.0  116.0  18.0  28.5  0.219  22.0\n",
       "743   9.0  140.0  94.0    0.0   0.0  32.7  0.734  45.0\n",
       "744  13.0  153.0  88.0  140.0  37.0  40.6  1.174  39.0\n",
       "745  12.0  100.0  84.0  105.0  33.0  30.0  0.488  46.0\n",
       "746   1.0  147.0  94.0    0.0  41.0  49.3  0.358  27.0\n",
       "747   1.0   81.0  74.0   57.0  41.0  46.3  1.096  32.0\n",
       "748   3.0  187.0  70.0  200.0  22.0  36.4  0.408  36.0\n",
       "749   6.0  162.0  62.0    0.0   0.0  24.3  0.178  50.0\n",
       "750   4.0  136.0  70.0    0.0   0.0  31.2  1.182  22.0\n",
       "751   1.0  121.0  78.0   74.0  39.0  39.0  0.261  28.0\n",
       "752   3.0  108.0  62.0    0.0  24.0  26.0  0.223  25.0\n",
       "753   0.0  181.0  88.0  510.0  44.0  43.3  0.222  26.0\n",
       "754   8.0  154.0  78.0    0.0  32.0  32.4  0.443  45.0\n",
       "755   1.0  128.0  88.0  110.0  39.0  36.5  1.057  37.0\n",
       "756   7.0  137.0  90.0    0.0  41.0  32.0  0.391  39.0\n",
       "757   0.0  123.0  72.0    0.0   0.0  36.3  0.258  52.0\n",
       "758   1.0  106.0  76.0    0.0   0.0  37.5  0.197  26.0\n",
       "759   6.0  190.0  92.0    0.0   0.0  35.5  0.278  66.0\n",
       "760   2.0   88.0  58.0   16.0  26.0  28.4  0.766  22.0\n",
       "761   9.0  170.0  74.0    0.0  31.0  44.0  0.403  43.0\n",
       "762   9.0   89.0  62.0    0.0   0.0  22.5  0.142  33.0\n",
       "763  10.0  101.0  76.0  180.0  48.0  32.9  0.171  63.0\n",
       "764   2.0  122.0  70.0    0.0  27.0  36.8  0.340  27.0\n",
       "765   5.0  121.0  72.0  112.0  23.0  26.2  0.245  30.0\n",
       "766   1.0  126.0  60.0    0.0   0.0  30.1  0.349  47.0\n",
       "767   1.0   93.0  70.0    0.0  31.0  30.4  0.315  23.0\n",
       "\n",
       "[768 rows x 8 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(X)\n",
    "np.where(np.isnan(X))\n",
    "np.nan_to_num(X)\n",
    "pd.DataFrame(X).fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    " # Create Gaussian naive bayes model object and train it with the data  \n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7542\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_predict_train = nb_model.predict(X_train) \n",
    " # import the performance metrices library\n",
    "from sklearn import metrics \n",
    " # Accuracy\n",
    "print(\"Accuracy : {0:.4f}\".format(metrics.accuracy_score(y_train, nb_predict_train)))\n",
    "print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_predict_text = nb_model.predict(X_test) \n",
    " # import the performance metrices library\n",
    "from sklearn import metrics \n",
    " # Accuracy\n",
    "print(\"Accuracy : {0:.4f}\".format(metrics.accuracy_score(y_test, nb_predict_text)))\n",
    "print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[ 52  28]\n",
      " [ 33 118]]\n",
      " \n",
      " Classification Report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.65      0.63        80\n",
      "           0       0.81      0.78      0.79       151\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       231\n",
      "   macro avg       0.71      0.72      0.71       231\n",
      "weighted avg       0.74      0.74      0.74       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix\") \n",
    " # the use of labels to set 1=True to upper left and 0=False to lower right \n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_test, nb_predict_text, labels = [1,0]))) \n",
    "print(\" \") \n",
    "print(\" Classification Report \") \n",
    "print(metrics.classification_report(y_test, nb_predict_text, labels = [1,0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(random_state = 42) # create random forest object\n",
    "rf_model.fit(X_train, y_train.ravel()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9907\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_predict_train = rf_model.predict(X_train) # training metrics \n",
    "print(\"Accuracy : {0:.4f}\".format(metrics.accuracy_score(y_train, rf_predict_train))) \n",
    "print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_predict_test = rf_model.predict(X_test) # training metrics \n",
    "print(\"Accuracy : {0:.4f}\".format(metrics.accuracy_score(y_test, rf_predict_test)))\n",
    "print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[ 51  29]\n",
      " [ 27 124]]\n",
      " \n",
      " Classification Report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.64      0.65        80\n",
      "           0       0.81      0.82      0.82       151\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       231\n",
      "   macro avg       0.73      0.73      0.73       231\n",
      "weighted avg       0.76      0.76      0.76       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix\") \n",
    " # the use of labels to set 1=True to upper left and 0=False to lower right \n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_test, rf_predict_test, labels = [1,0])))\n",
    "print(\" \") \n",
    "print(\" Classification Report \")\n",
    "print(metrics.classification_report(y_test, rf_predict_test, labels = [1,0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.7, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=42, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "lr_model = LogisticRegression(C=0.7, random_state=42) # create random forest object\n",
    "lr_model.fit(X_train, y_train.ravel()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7446\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_predict_test = lr_model.predict(X_test) # training metrics \n",
    "print(\"Accuracy : {0:.4f}\".format(metrics.accuracy_score(y_test, lr_predict_test)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[ 44  36]\n",
      " [ 23 128]]\n",
      " \n",
      " Classification Report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.55      0.60        80\n",
      "           0       0.78      0.85      0.81       151\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       231\n",
      "   macro avg       0.72      0.70      0.71       231\n",
      "weighted avg       0.74      0.74      0.74       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix\") \n",
    " # the use of labels to set 1=True to upper left and 0=False to lower right \n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_test, lr_predict_test, labels = [1,0])))\n",
    "print(\" \") \n",
    "print(\" Classification Report \")\n",
    "print(metrics.classification_report(y_test, lr_predict_test, labels = [1,0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st max value of 0.613 occured at c=1.400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'recall score')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "C_start = 0.1\n",
    "C_end = 5 \n",
    "C_inc = 0.1 \n",
    "\n",
    "\n",
    "C_values, recall_scores = [] , []\n",
    "C_val = C_start \n",
    "best_recall_score = 0 \n",
    "\n",
    "while (C_val < C_end): \n",
    "    C_values.append(C_val)   \n",
    "    lr_model_loop = LogisticRegression(C=C_val , random_state=42)   \n",
    "    lr_model_loop.fit(X_train , y_train.ravel())  \n",
    "    lr_predict_loop_test = lr_model_loop.predict(X_test)  \n",
    "    recall_score = metrics.recall_score(y_test, lr_predict_loop_test)  \n",
    "    recall_scores.append(recall_score)  \n",
    "    if (recall_score > best_recall_score):    \n",
    "        best_recall_score = recall_score      \n",
    "        best_lr_predict_test = lr_predict_loop_test        \n",
    "       \n",
    "    C_val = C_val + C_inc  \n",
    "       \n",
    "best_score_C_val = C_values[recall_scores.index(best_recall_score)]\n",
    "print(\"1st max value of {0:.3f} occured at c={1:.3f}\".format(best_recall_score,best_score_C_val)) \n",
    "\n",
    "%matplotlib inline \n",
    "plt.plot(C_values , recall_scores, \"-\")\n",
    "plt.xlabel(\"C value\") \n",
    "plt.ylabel(\"recall score\") \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.4000000000000001, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=42,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_model = LogisticRegression(class_weight='balanced' , C=best_score_C_val, random_state=42)  \n",
    "lr_model.fit(X_train, y_train.ravel()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7056\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_predict_test = lr_model.predict(X_test) # training metrics\n",
    "print(\"Accuracy : {0:.4f}\".format(metrics.accuracy_score(y_test, lr_predict_test)))\n",
    "print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[ 55  25]\n",
      " [ 43 108]]\n",
      " \n",
      " Classification Report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.69      0.62        80\n",
      "           0       0.81      0.72      0.76       151\n",
      "\n",
      "   micro avg       0.71      0.71      0.71       231\n",
      "   macro avg       0.69      0.70      0.69       231\n",
      "weighted avg       0.73      0.71      0.71       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix\") \n",
    " # the use of labels to set 1=True to upper left and 0=False to lower right \n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_test, lr_predict_test, labels = [1,0])))\n",
    "print(\" \") \n",
    "print(\" Classification Report \") \n",
    "print(metrics.classification_report(y_test, lr_predict_test, labels = [1,0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=3, class_weight='balanced', cv=10, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "           multi_class='warn', n_jobs=-1, penalty='l2', random_state=42,\n",
       "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "lr_cv_model = LogisticRegressionCV(n_jobs=-1, random_state=42, Cs=3, cv=10, refit=True, class_weight='balanced') \n",
    "lr_cv_model.fit(X_train, y_train.ravel()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_cv_predict_test = lr_cv_model.predict(X_test) # training metrics\n",
    "print(\"Accuracy : {0:.4f}\".format(metrics.accuracy_score(y_test, lr_cv_predict_test))) \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[ 53  27]\n",
      " [ 42 109]]\n",
      " \n",
      " Classification Report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.66      0.61        80\n",
      "           0       0.80      0.72      0.76       151\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       231\n",
      "   macro avg       0.68      0.69      0.68       231\n",
      "weighted avg       0.72      0.70      0.71       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix\") \n",
    " # the use of labels to set 1=True to upper left and 0=False to lower right \n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_test, lr_cv_predict_test, labels = [1,0])))\n",
    "print(\" \") \n",
    "print(\" Classification Report \") \n",
    "print(metrics.classification_report(y_test, lr_cv_predict_test, labels = [1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
