{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                    # pandas is a dataframe library\n",
    "import matplotlib.pyplot as plt        # matplotlib.pyplot plot data \n",
    "import numpy as np           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " df = pd.read_csv(\"D:\\diabetes_csv.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>insu</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preg  plas  pres  skin  insu  mass   pedi  age  class\n",
       "0     6   148    72    35     0  33.6  0.627   50   True\n",
       "1     1    85    66    29     0  26.6  0.351   31  False\n",
       "2     8   183    64     0     0  23.3  0.672   32   True\n",
       "3     1    89    66    23    94  28.1  0.167   21  False\n",
       "4     0   137    40    35   168  43.1  2.288   33   True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>insu</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     preg  plas  pres  skin  insu  mass   pedi  age  class\n",
       "763    10   101    76    48   180  32.9  0.171   63  False\n",
       "764     2   122    70    27     0  36.8  0.340   27  False\n",
       "765     5   121    72    23   112  26.2  0.245   30  False\n",
       "766     1   126    60     0     0  30.1  0.349   47   True\n",
       "767     1    93    70    31     0  30.4  0.315   23  False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df.isnull().values.any() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(df, size=10): \n",
    "    corr = df.corr()  # data frame corelation function  \n",
    "    fig , ax = plt.subplots(figsize =(size,size))   \n",
    "    ax.matshow(corr)  # color code the rectanges by corelation value    \n",
    "    plt.xticks (range(len(corr.columns)), corr.columns)    # draw x ticks mark   \n",
    "    plt.yticks (range(len(corr.columns)), corr.columns)    # draw y ticks mark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x792 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " plot_corr(df) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>insu</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>preg</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.129459</td>\n",
       "      <td>0.141282</td>\n",
       "      <td>-0.081672</td>\n",
       "      <td>-0.073535</td>\n",
       "      <td>0.017683</td>\n",
       "      <td>-0.033523</td>\n",
       "      <td>0.544341</td>\n",
       "      <td>0.221898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plas</th>\n",
       "      <td>0.129459</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.152590</td>\n",
       "      <td>0.057328</td>\n",
       "      <td>0.331357</td>\n",
       "      <td>0.221071</td>\n",
       "      <td>0.137337</td>\n",
       "      <td>0.263514</td>\n",
       "      <td>0.466581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pres</th>\n",
       "      <td>0.141282</td>\n",
       "      <td>0.152590</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.207371</td>\n",
       "      <td>0.088933</td>\n",
       "      <td>0.281805</td>\n",
       "      <td>0.041265</td>\n",
       "      <td>0.239528</td>\n",
       "      <td>0.065068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skin</th>\n",
       "      <td>-0.081672</td>\n",
       "      <td>0.057328</td>\n",
       "      <td>0.207371</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.436783</td>\n",
       "      <td>0.392573</td>\n",
       "      <td>0.183928</td>\n",
       "      <td>-0.113970</td>\n",
       "      <td>0.074752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insu</th>\n",
       "      <td>-0.073535</td>\n",
       "      <td>0.331357</td>\n",
       "      <td>0.088933</td>\n",
       "      <td>0.436783</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.197859</td>\n",
       "      <td>0.185071</td>\n",
       "      <td>-0.042163</td>\n",
       "      <td>0.130548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mass</th>\n",
       "      <td>0.017683</td>\n",
       "      <td>0.221071</td>\n",
       "      <td>0.281805</td>\n",
       "      <td>0.392573</td>\n",
       "      <td>0.197859</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.140647</td>\n",
       "      <td>0.036242</td>\n",
       "      <td>0.292695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pedi</th>\n",
       "      <td>-0.033523</td>\n",
       "      <td>0.137337</td>\n",
       "      <td>0.041265</td>\n",
       "      <td>0.183928</td>\n",
       "      <td>0.185071</td>\n",
       "      <td>0.140647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033561</td>\n",
       "      <td>0.173844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.544341</td>\n",
       "      <td>0.263514</td>\n",
       "      <td>0.239528</td>\n",
       "      <td>-0.113970</td>\n",
       "      <td>-0.042163</td>\n",
       "      <td>0.036242</td>\n",
       "      <td>0.033561</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.238356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <td>0.221898</td>\n",
       "      <td>0.466581</td>\n",
       "      <td>0.065068</td>\n",
       "      <td>0.074752</td>\n",
       "      <td>0.130548</td>\n",
       "      <td>0.292695</td>\n",
       "      <td>0.173844</td>\n",
       "      <td>0.238356</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           preg      plas      pres      skin      insu      mass      pedi  \\\n",
       "preg   1.000000  0.129459  0.141282 -0.081672 -0.073535  0.017683 -0.033523   \n",
       "plas   0.129459  1.000000  0.152590  0.057328  0.331357  0.221071  0.137337   \n",
       "pres   0.141282  0.152590  1.000000  0.207371  0.088933  0.281805  0.041265   \n",
       "skin  -0.081672  0.057328  0.207371  1.000000  0.436783  0.392573  0.183928   \n",
       "insu  -0.073535  0.331357  0.088933  0.436783  1.000000  0.197859  0.185071   \n",
       "mass   0.017683  0.221071  0.281805  0.392573  0.197859  1.000000  0.140647   \n",
       "pedi  -0.033523  0.137337  0.041265  0.183928  0.185071  0.140647  1.000000   \n",
       "age    0.544341  0.263514  0.239528 -0.113970 -0.042163  0.036242  0.033561   \n",
       "class  0.221898  0.466581  0.065068  0.074752  0.130548  0.292695  0.173844   \n",
       "\n",
       "            age     class  \n",
       "preg   0.544341  0.221898  \n",
       "plas   0.263514  0.466581  \n",
       "pres   0.239528  0.065068  \n",
       "skin  -0.113970  0.074752  \n",
       "insu  -0.042163  0.130548  \n",
       "mass   0.036242  0.292695  \n",
       "pedi   0.033561  0.173844  \n",
       "age    1.000000  0.238356  \n",
       "class  0.238356  1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df.corr() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>insu</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preg  plas  pres  skin  insu  mass   pedi  age  class\n",
       "0     6   148    72    35     0  33.6  0.627   50   True\n",
       "1     1    85    66    29     0  26.6  0.351   31  False\n",
       "2     8   183    64     0     0  23.3  0.672   32   True\n",
       "3     1    89    66    23    94  28.1  0.167   21  False\n",
       "4     0   137    40    35   168  43.1  2.288   33   True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " class_map = {True:1 , False:0} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " df['class'] = df['class'].map(class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>insu</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preg  plas  pres  skin  insu  mass   pedi  age  class\n",
       "0     6   148    72    35     0  33.6  0.627   50      1\n",
       "1     1    85    66    29     0  26.6  0.351   31      0\n",
       "2     8   183    64     0     0  23.3  0.672   32      1\n",
       "3     1    89    66    23    94  28.1  0.167   21      0\n",
       "4     0   137    40    35   168  43.1  2.288   33      1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df.head(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "feature_col_names = ['preg', 'plas' , 'pres', 'insu', 'skin', 'mass', 'pedi', 'age']\n",
    "predicted_class_names = ['class'] \n",
    "X = df[feature_col_names].values # predictor feature coloumns ( 8 X m )\n",
    "y = df[predicted_class_names].values # predicted class ( 1= true , 0=false ) column ( 1 X m )\n",
    "split_test_size = 0.30 \n",
    "X_train , X_test , y_train , y_test = train_test_split(X, y, test_size = split_test_size, random_state = 42)           # test size = 0.30 is 30% , 42 is the answer to everything , any number can be used for ran dom state \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.92% in training set\n",
      "30.08% in test set\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:0.2f}% in training set\".format((len(X_train)/len(df.index))*100))\n",
    "print(\"{0:0.2f}% in test set\".format((len(X_test)/len(df.index))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origional True: 268 (34.90%)\n",
      "Origional False: 500 (65.10%)\n",
      " \n",
      "Training True: 188 (35.01%)\n",
      "Training False: 349 (64.99%)\n",
      " \n",
      "Testing True: 80 (34.63%)\n",
      "Testing False: 151 (65.37%)\n"
     ]
    }
   ],
   "source": [
    " print (\"Origional True: {0} ({1:0.2f}%)\".format(len(df.loc[df['class'] == 1]),(len(df.loc[df['class'] == 1]) / len(df.index))*100.0)) \n",
    "print (\"Origional False: {0} ({1:0.2f}%)\".format(len(df.loc[df['class'] == 0]),(len(df.loc[df['class'] == 0]) / len(df.index))*100.0))\n",
    "print(\" \")\n",
    "print (\"Training True: {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 1]),(len(y_train[y_train[:] == 1]) / len(y_train))*100.0))\n",
    "print (\"Training False: {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 0]),(len(y_train[y_train[:] == 0]) / len(y_train))*100.0))\n",
    "print(\" \")\n",
    "print (\"Testing True: {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 1]),(len(y_test[y_test[:] == 1]) / len(y_test))*100.0)) \n",
    "print (\"Testing False: {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 0]),(len(y_test[y_test[:] == 0]) / len(y_test))*100.0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows in dataframe 768\n",
      "# rows missing glucose_conc: 5\n",
      "# rows missing diastolic_bp: 35\n",
      "# rows missing thickness: 227\n",
      "# rows missing insulin: 374\n",
      "# rows missing bmi: 11\n",
      "# rows missing diab_pred: 0\n",
      "# rows missing age: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"# rows in dataframe {0}\".format(len(df))) \n",
    "print(\"# rows missing glucose_conc: {0}\".format(len(df.loc[df['plas'] == 0 ]))) \n",
    "print(\"# rows missing diastolic_bp: {0}\".format(len(df.loc[df['pres'] == 0 ])))\n",
    "print(\"# rows missing thickness: {0}\".format(len(df.loc[df['skin'] == 0 ]))) \n",
    "print(\"# rows missing insulin: {0}\".format(len(df.loc[df['insu'] == 0 ]))) \n",
    "print(\"# rows missing bmi: {0}\".format(len(df.loc[df['mass'] == 0 ])))\n",
    "print(\"# rows missing diab_pred: {0}\".format(len(df.loc[df['pedi'] == 0 ]))) \n",
    "print(\"# rows missing age: {0}\".format(len(df.loc[df['age'] == 0 ]))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer #impute with mean all 0 readings \n",
    "fill_0 = Imputer(missing_values = 0 , strategy =\"mean\", axis=0) \n",
    "X_train = fill_0.fit_transform(X_train) \n",
    "X_test = fill_0.fit_transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>543.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.191</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.537</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.1</td>\n",
       "      <td>1.441</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>846.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.587</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.484</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0.551</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.183</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>34.6</td>\n",
       "      <td>0.529</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>39.3</td>\n",
       "      <td>0.704</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.4</td>\n",
       "      <td>0.388</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>0.451</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.263</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>11.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>36.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>31.1</td>\n",
       "      <td>0.205</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.4</td>\n",
       "      <td>0.257</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>0.487</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.1</td>\n",
       "      <td>0.337</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>2.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>36.6</td>\n",
       "      <td>0.453</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>1.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.5</td>\n",
       "      <td>0.293</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>11.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>42.3</td>\n",
       "      <td>0.785</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>3.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.8</td>\n",
       "      <td>0.400</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>1.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>0.219</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>9.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.7</td>\n",
       "      <td>0.734</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>13.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1.174</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>12.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.488</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>1.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>49.3</td>\n",
       "      <td>0.358</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>1.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>46.3</td>\n",
       "      <td>1.096</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>3.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>36.4</td>\n",
       "      <td>0.408</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>6.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.178</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>4.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.2</td>\n",
       "      <td>1.182</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>1.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.261</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>3.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.223</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>0.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.222</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>8.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.4</td>\n",
       "      <td>0.443</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>1.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>1.057</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>7.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.391</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>0.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.3</td>\n",
       "      <td>0.258</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>1.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>0.197</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>6.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.5</td>\n",
       "      <td>0.278</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>2.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>28.4</td>\n",
       "      <td>0.766</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>9.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.403</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>9.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>0.142</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1     2      3     4     5      6     7\n",
       "0     6.0  148.0  72.0    0.0  35.0  33.6  0.627  50.0\n",
       "1     1.0   85.0  66.0    0.0  29.0  26.6  0.351  31.0\n",
       "2     8.0  183.0  64.0    0.0   0.0  23.3  0.672  32.0\n",
       "3     1.0   89.0  66.0   94.0  23.0  28.1  0.167  21.0\n",
       "4     0.0  137.0  40.0  168.0  35.0  43.1  2.288  33.0\n",
       "5     5.0  116.0  74.0    0.0   0.0  25.6  0.201  30.0\n",
       "6     3.0   78.0  50.0   88.0  32.0  31.0  0.248  26.0\n",
       "7    10.0  115.0   0.0    0.0   0.0  35.3  0.134  29.0\n",
       "8     2.0  197.0  70.0  543.0  45.0  30.5  0.158  53.0\n",
       "9     8.0  125.0  96.0    0.0   0.0   0.0  0.232  54.0\n",
       "10    4.0  110.0  92.0    0.0   0.0  37.6  0.191  30.0\n",
       "11   10.0  168.0  74.0    0.0   0.0  38.0  0.537  34.0\n",
       "12   10.0  139.0  80.0    0.0   0.0  27.1  1.441  57.0\n",
       "13    1.0  189.0  60.0  846.0  23.0  30.1  0.398  59.0\n",
       "14    5.0  166.0  72.0  175.0  19.0  25.8  0.587  51.0\n",
       "15    7.0  100.0   0.0    0.0   0.0  30.0  0.484  32.0\n",
       "16    0.0  118.0  84.0  230.0  47.0  45.8  0.551  31.0\n",
       "17    7.0  107.0  74.0    0.0   0.0  29.6  0.254  31.0\n",
       "18    1.0  103.0  30.0   83.0  38.0  43.3  0.183  33.0\n",
       "19    1.0  115.0  70.0   96.0  30.0  34.6  0.529  32.0\n",
       "20    3.0  126.0  88.0  235.0  41.0  39.3  0.704  27.0\n",
       "21    8.0   99.0  84.0    0.0   0.0  35.4  0.388  50.0\n",
       "22    7.0  196.0  90.0    0.0   0.0  39.8  0.451  41.0\n",
       "23    9.0  119.0  80.0    0.0  35.0  29.0  0.263  29.0\n",
       "24   11.0  143.0  94.0  146.0  33.0  36.6  0.254  51.0\n",
       "25   10.0  125.0  70.0  115.0  26.0  31.1  0.205  41.0\n",
       "26    7.0  147.0  76.0    0.0   0.0  39.4  0.257  43.0\n",
       "27    1.0   97.0  66.0  140.0  15.0  23.2  0.487  22.0\n",
       "28   13.0  145.0  82.0  110.0  19.0  22.2  0.245  57.0\n",
       "29    5.0  117.0  92.0    0.0   0.0  34.1  0.337  38.0\n",
       "..    ...    ...   ...    ...   ...   ...    ...   ...\n",
       "738   2.0   99.0  60.0  160.0  17.0  36.6  0.453  21.0\n",
       "739   1.0  102.0  74.0    0.0   0.0  39.5  0.293  42.0\n",
       "740  11.0  120.0  80.0  150.0  37.0  42.3  0.785  48.0\n",
       "741   3.0  102.0  44.0   94.0  20.0  30.8  0.400  26.0\n",
       "742   1.0  109.0  58.0  116.0  18.0  28.5  0.219  22.0\n",
       "743   9.0  140.0  94.0    0.0   0.0  32.7  0.734  45.0\n",
       "744  13.0  153.0  88.0  140.0  37.0  40.6  1.174  39.0\n",
       "745  12.0  100.0  84.0  105.0  33.0  30.0  0.488  46.0\n",
       "746   1.0  147.0  94.0    0.0  41.0  49.3  0.358  27.0\n",
       "747   1.0   81.0  74.0   57.0  41.0  46.3  1.096  32.0\n",
       "748   3.0  187.0  70.0  200.0  22.0  36.4  0.408  36.0\n",
       "749   6.0  162.0  62.0    0.0   0.0  24.3  0.178  50.0\n",
       "750   4.0  136.0  70.0    0.0   0.0  31.2  1.182  22.0\n",
       "751   1.0  121.0  78.0   74.0  39.0  39.0  0.261  28.0\n",
       "752   3.0  108.0  62.0    0.0  24.0  26.0  0.223  25.0\n",
       "753   0.0  181.0  88.0  510.0  44.0  43.3  0.222  26.0\n",
       "754   8.0  154.0  78.0    0.0  32.0  32.4  0.443  45.0\n",
       "755   1.0  128.0  88.0  110.0  39.0  36.5  1.057  37.0\n",
       "756   7.0  137.0  90.0    0.0  41.0  32.0  0.391  39.0\n",
       "757   0.0  123.0  72.0    0.0   0.0  36.3  0.258  52.0\n",
       "758   1.0  106.0  76.0    0.0   0.0  37.5  0.197  26.0\n",
       "759   6.0  190.0  92.0    0.0   0.0  35.5  0.278  66.0\n",
       "760   2.0   88.0  58.0   16.0  26.0  28.4  0.766  22.0\n",
       "761   9.0  170.0  74.0    0.0  31.0  44.0  0.403  43.0\n",
       "762   9.0   89.0  62.0    0.0   0.0  22.5  0.142  33.0\n",
       "763  10.0  101.0  76.0  180.0  48.0  32.9  0.171  63.0\n",
       "764   2.0  122.0  70.0    0.0  27.0  36.8  0.340  27.0\n",
       "765   5.0  121.0  72.0  112.0  23.0  26.2  0.245  30.0\n",
       "766   1.0  126.0  60.0    0.0   0.0  30.1  0.349  47.0\n",
       "767   1.0   93.0  70.0    0.0  31.0  30.4  0.315  23.0\n",
       "\n",
       "[768 rows x 8 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(X)\n",
    "np.where(np.isnan(X))\n",
    "np.nan_to_num(X)\n",
    "pd.DataFrame(X).fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    " # Create Gaussian naive bayes model object and train it with the data  \n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7542\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_predict_train = nb_model.predict(X_train) \n",
    " # import the performance metrices library\n",
    "from sklearn import metrics \n",
    " # Accuracy\n",
    "print(\"Accuracy : {0:.4f}\".format(metrics.accuracy_score(y_train, nb_predict_train)))\n",
    "print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_predict_text = nb_model.predict(X_test) \n",
    " # import the performance metrices library\n",
    "from sklearn import metrics \n",
    " # Accuracy\n",
    "print(\"Accuracy : {0:.4f}\".format(metrics.accuracy_score(y_test, nb_predict_text)))\n",
    "print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[ 52  28]\n",
      " [ 33 118]]\n",
      " \n",
      " Classification Report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.65      0.63        80\n",
      "           0       0.81      0.78      0.79       151\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       231\n",
      "   macro avg       0.71      0.72      0.71       231\n",
      "weighted avg       0.74      0.74      0.74       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix\") \n",
    " # the use of labels to set 1=True to upper left and 0=False to lower right \n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_test, nb_predict_text, labels = [1,0]))) \n",
    "print(\" \") \n",
    "print(\" Classification Report \") \n",
    "print(metrics.classification_report(y_test, nb_predict_text, labels = [1,0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(random_state = 42) # create random forest object\n",
    "rf_model.fit(X_train, y_train.ravel()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9907\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_predict_train = rf_model.predict(X_train) # training metrics \n",
    "print(\"Accuracy : {0:.4f}\".format(metrics.accuracy_score(y_train, rf_predict_train))) \n",
    "print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_predict_test = rf_model.predict(X_test) # training metrics \n",
    "print(\"Accuracy : {0:.4f}\".format(metrics.accuracy_score(y_test, rf_predict_test)))\n",
    "print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[ 51  29]\n",
      " [ 27 124]]\n",
      " \n",
      " Classification Report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.64      0.65        80\n",
      "           0       0.81      0.82      0.82       151\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       231\n",
      "   macro avg       0.73      0.73      0.73       231\n",
      "weighted avg       0.76      0.76      0.76       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix\") \n",
    " # the use of labels to set 1=True to upper left and 0=False to lower right \n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_test, rf_predict_test, labels = [1,0])))\n",
    "print(\" \") \n",
    "print(\" Classification Report \")\n",
    "print(metrics.classification_report(y_test, rf_predict_test, labels = [1,0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.7, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=42, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "lr_model = LogisticRegression(C=0.7, random_state=42) # create random forest object\n",
    "lr_model.fit(X_train, y_train.ravel()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7446\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_predict_test = lr_model.predict(X_test) # training metrics \n",
    "print(\"Accuracy : {0:.4f}\".format(metrics.accuracy_score(y_test, lr_predict_test)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[ 44  36]\n",
      " [ 23 128]]\n",
      " \n",
      " Classification Report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.55      0.60        80\n",
      "           0       0.78      0.85      0.81       151\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       231\n",
      "   macro avg       0.72      0.70      0.71       231\n",
      "weighted avg       0.74      0.74      0.74       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix\") \n",
    " # the use of labels to set 1=True to upper left and 0=False to lower right \n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_test, lr_predict_test, labels = [1,0])))\n",
    "print(\" \") \n",
    "print(\" Classification Report \")\n",
    "print(metrics.classification_report(y_test, lr_predict_test, labels = [1,0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st max value of 0.613 occured at c=1.400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'recall score')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "C_start = 0.1\n",
    "C_end = 5 \n",
    "C_inc = 0.1 \n",
    "\n",
    "\n",
    "C_values, recall_scores = [] , []\n",
    "C_val = C_start \n",
    "best_recall_score = 0 \n",
    "\n",
    "while (C_val < C_end): \n",
    "    C_values.append(C_val)   \n",
    "    lr_model_loop = LogisticRegression(C=C_val , random_state=42)   \n",
    "    lr_model_loop.fit(X_train , y_train.ravel())  \n",
    "    lr_predict_loop_test = lr_model_loop.predict(X_test)  \n",
    "    recall_score = metrics.recall_score(y_test, lr_predict_loop_test)  \n",
    "    recall_scores.append(recall_score)  \n",
    "    if (recall_score > best_recall_score):    \n",
    "        best_recall_score = recall_score      \n",
    "        best_lr_predict_test = lr_predict_loop_test        \n",
    "       \n",
    "    C_val = C_val + C_inc  \n",
    "       \n",
    "best_score_C_val = C_values[recall_scores.index(best_recall_score)]\n",
    "print(\"1st max value of {0:.3f} occured at c={1:.3f}\".format(best_recall_score,best_score_C_val)) \n",
    "\n",
    "%matplotlib inline \n",
    "plt.plot(C_values , recall_scores, \"-\")\n",
    "plt.xlabel(\"C value\") \n",
    "plt.ylabel(\"recall score\") \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.30000000000000004, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=42,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_model = LogisticRegression(class_weight='balanced' , C=best_score_C_val, random_state=42)  \n",
    "lr_model.fit(X_train, y_train.ravel()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_predict_test = lr_model.predict(X_test) # training metrics\n",
    "print(\"Accuracy : {0:.4f}\".format(metrics.accuracy_score(y_test, lr_predict_test)))\n",
    "print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[ 59  21]\n",
      " [ 45 106]]\n",
      " \n",
      " Classification Report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.57      0.74      0.64        80\n",
      "           0       0.83      0.70      0.76       151\n",
      "\n",
      "   micro avg       0.71      0.71      0.71       231\n",
      "   macro avg       0.70      0.72      0.70       231\n",
      "weighted avg       0.74      0.71      0.72       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix\") \n",
    " # the use of labels to set 1=True to upper left and 0=False to lower right \n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_test, lr_predict_test, labels = [1,0])))\n",
    "print(\" \") \n",
    "print(\" Classification Report \") \n",
    "print(metrics.classification_report(y_test, lr_predict_test, labels = [1,0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vadhana\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=3, class_weight='balanced', cv=10, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "           multi_class='warn', n_jobs=-1, penalty='l2', random_state=42,\n",
       "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "lr_cv_model = LogisticRegressionCV(n_jobs=-1, random_state=42, Cs=3, cv=10, refit=True, class_weight='balanced') \n",
    "lr_cv_model.fit(X_train, y_train.ravel()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_cv_predict_test = lr_cv_model.predict(X_test) # training metrics\n",
    "print(\"Accuracy : {0:.4f}\".format(metrics.accuracy_score(y_test, lr_cv_predict_test))) \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[ 53  27]\n",
      " [ 42 109]]\n",
      " \n",
      " Classification Report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.66      0.61        80\n",
      "           0       0.80      0.72      0.76       151\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       231\n",
      "   macro avg       0.68      0.69      0.68       231\n",
      "weighted avg       0.72      0.70      0.71       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix\") \n",
    " # the use of labels to set 1=True to upper left and 0=False to lower right \n",
    "print(\"{0}\".format(metrics.confusion_matrix(y_test, lr_cv_predict_test, labels = [1,0])))\n",
    "print(\" \") \n",
    "print(\" Classification Report \") \n",
    "print(metrics.classification_report(y_test, lr_cv_predict_test, labels = [1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
